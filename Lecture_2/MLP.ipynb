{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e612435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks 2 - Multi Layer Perceptrons or Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7782430-8d2f-4cbd-8905-84960f5dbeb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This week's task:\n",
    "## Train a Neural Network to predict the grades of a metal alloy based on a series of parameters\n",
    "\n",
    "### Description\n",
    "\n",
    "During the metal manufacturing process, a range of parameters can affect the grade, and therefore the potential uses, of the resulting alloy. Given values for these parameters, we would like to be able to predict the grade of the resulting alloy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35dd6f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The dataset\n",
    "\n",
    "The dataset included in the accompanying `Metal_Grade.csv` file is a slightly modified version of the dataset from this [link](https://www.kaggle.com/esotericazzo/metal-furnace-dataset) and includes 27 factors affecting the quality of manufactured alloys, along with the resulting grade for 620 cases. The file can be visualised with `pandas` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f173679e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.848564</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>2.329398</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>0.443257</td>\n",
       "      <td>-0.406121</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>3.727218</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>3.032397</td>\n",
       "      <td>-2.442599</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>-0.232546</td>\n",
       "      <td>-0.406366</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.848564</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>2.329398</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>1.459782</td>\n",
       "      <td>1.221876</td>\n",
       "      <td>1.877777</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511733</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>-0.008030</td>\n",
       "      <td>-0.406366</td>\n",
       "      <td>1.504523</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>-2.526055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>-0.573268</td>\n",
       "      <td>-1.164793</td>\n",
       "      <td>1.877777</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>2.004803</td>\n",
       "      <td>-2.442599</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>-0.684962</td>\n",
       "      <td>-0.406121</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>-0.379487</td>\n",
       "      <td>3.809407</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>2.248407</td>\n",
       "      <td>-0.332678</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>4.519156</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>-1.023427</td>\n",
       "      <td>1.332042</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.264250</td>\n",
       "      <td>2.004803</td>\n",
       "      <td>-2.442599</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>3.150982</td>\n",
       "      <td>-1.777067</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>-0.379487</td>\n",
       "      <td>3.068742</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>1.120188</td>\n",
       "      <td>0.303833</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f0        f1        f2        f3        f4        f5        f6  \\\n",
       "0    1.848564 -0.264250 -0.461423  0.409400  1.305455  2.329398  0.370965   \n",
       "1   -0.825098 -0.264250  3.032397 -2.442599  1.305455 -0.276144  0.370965   \n",
       "2    1.848564 -0.264250 -0.461423  0.409400  1.305455  2.329398  0.370965   \n",
       "3    0.511733 -0.264250 -0.461423  0.409400 -0.525726 -0.276144  0.370965   \n",
       "4   -0.825098 -0.264250 -0.461423  0.409400 -0.525726 -0.276144  0.370965   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "615 -0.825098 -0.264250  2.004803 -2.442599  1.305455 -0.276144  0.370965   \n",
       "616 -0.379487  3.809407 -0.461423  0.409400  1.305455 -0.276144  0.370965   \n",
       "617 -0.825098 -0.264250 -0.461423  0.409400 -0.525726 -0.276144  0.370965   \n",
       "618 -0.825098 -0.264250  2.004803 -2.442599  1.305455 -0.276144  0.370965   \n",
       "619 -0.379487  3.068742 -0.461423  0.409400  1.305455 -0.276144  0.370965   \n",
       "\n",
       "           f7        f8        f9  ...       f18       f19       f20  \\\n",
       "0    0.090167  0.107958  0.395874  ...  0.085505  0.233285 -1.080663   \n",
       "1    0.090167  0.107958  0.395874  ...  0.085505  0.233285 -1.080663   \n",
       "2    0.090167  0.107958  0.395874  ...  0.085505  0.233285  0.925358   \n",
       "3    0.090167  0.107958  0.395874  ...  0.085505  0.233285  0.925358   \n",
       "4    0.090167  0.107958 -2.526055  ...  0.085505  0.233285  0.925358   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "615  0.090167  0.107958  0.395874  ...  0.085505  0.233285 -1.080663   \n",
       "616  0.090167  0.107958  0.395874  ...  0.085505  0.233285 -1.080663   \n",
       "617  0.090167  0.107958  0.395874  ...  0.085505  0.233285 -1.080663   \n",
       "618  0.090167  0.107958  0.395874  ...  0.085505  0.233285 -1.080663   \n",
       "619  0.090167  0.107958  0.395874  ...  0.085505  0.233285 -1.080663   \n",
       "\n",
       "          f21       f22       f23       f24       f25       f26  grade  \n",
       "0    0.443257 -0.406121 -0.687687  0.271886  3.727218  0.102129      2  \n",
       "1   -0.232546 -0.406366 -0.687687  0.271886 -0.232472  0.102129      4  \n",
       "2    1.459782  1.221876  1.877777  0.271886 -0.232472  0.102129      2  \n",
       "3   -0.008030 -0.406366  1.504523  0.271886 -0.232472  0.102129      2  \n",
       "4   -0.573268 -1.164793  1.877777  0.271886 -0.232472  0.102129      2  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "615 -0.684962 -0.406121 -0.687687  0.271886 -0.232472  0.102129      2  \n",
       "616  2.248407 -0.332678 -0.687687  0.271886  4.519156  0.102129      2  \n",
       "617 -1.023427  1.332042 -0.687687  0.271886 -0.232472  0.102129      2  \n",
       "618  3.150982 -1.777067 -0.687687  0.271886 -0.232472  0.102129      2  \n",
       "619  1.120188  0.303833 -0.687687  0.271886 -0.232472  0.102129      2  \n",
       "\n",
       "[620 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd #import pandas module\n",
    "\n",
    "#pd.set_option('display.max_rows', None) #uncomment this line to view all rows\n",
    "#pd.set_option('display.max_columns', None) #uncomment this line to view all columns\n",
    "\n",
    "metalDf = pd.read_csv(\"Metal_Grade.csv\") #read the file containing the data\n",
    "\n",
    "metalDf #visualise data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60b1ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The SLP with multiple outputs\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The first issue that we encounter when trying to apply the SLP to more complicated problems, such as the one described above, is the number of outputs. \n",
    "\n",
    "The SLP, as described in the previous lecture, has a single output, which can be converted to binary format by applying an activation function and a threshold. However, this is only suitable for regression with one output, or binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cad0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A way to extend the SLP to more complicated problems, is by simply increasing the number of outputs, as illustrated below for a SLP with 3 inputs:\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/slp_2a.png\" width=\"600\" align=\"center\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b6a7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above figure introduces an alternative way of drawing neural networks, where:\n",
    "\n",
    "- An additional \"fictitious\" input with value 1 is considered.\n",
    "- The weights corresponding to this input are aquivalent to the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ab14b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, very often, this part is ommited, and networks are simply drawn as:\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/slp_2b.png\" width=\"600\" align=\"center\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f0cac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, notice that:\n",
    "\n",
    "- Every input is connected to every output, creating a so called fully connected layer\n",
    "- Weights have two indices:\n",
    "    + the first is related to the input they are connected to and\n",
    "    + the second is related to the output.\n",
    "- Biases also have an index, since a different bias is defined for each output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91575a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This architecture can be directly used for regression with multiple outputs. For classification problems, the outputs can be converted to values representing probabilities of the inputs belonging to each class through the use of a softmax activation function. \n",
    "\n",
    "The expression for the softmax function is: $$f\\left( \\mathbf{x} \\right)_i = \\dfrac{e^{x_i}}{\\Sigma_{j=1}^{n}e^{x_j}}$$ As an example, if the outputs of a MLP for classification using three classes are $y_1=12, y_2 = 16, y_3 = 17$, applying the softmax function will yield: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228945f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00490169 0.26762315 0.72747516]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''Softmax function'''\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "y = np.array([12., 16., 17.])\n",
    "\n",
    "yProb = softmax(y);\n",
    "\n",
    "print(yProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179095cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above can be interpreted as the input having 0.49% probability of belonging to class 1, 26.76% probability of belonging to class 2 and 72.74% probability of belonging to class 3. Very often we are only interested in knowing which class each input belongs to. This can be achieved by just extracting the class with the highest probability. Using numpy, this can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2fe794",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "yClass = np.argmax(yProb) #argmax returns the index of the maximum element of an array\n",
    "print(yClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d75920",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical expression\n",
    "\n",
    "For a general network with $n$ inputs and $m$ outputs, the operations performed by individual elements can be compactly described by the following expression:\n",
    "\n",
    "$ y_j = f\\left( \\Sigma_{i=1}^n w_{ij} x_j + b_j \\right)$\n",
    "\n",
    "where:\n",
    "\n",
    "+ $x_i$ are the inputs, for instance $x_1, x_2, \\dots$\n",
    "+ $y_j$ are the outputs, for instance $y_1, y_2, \\dots$\n",
    "+ $w_{ij}$ is the weight corresponding to input $i$ and output $j$\n",
    "+ $b_j$ is the bias corresponding to output $j$\n",
    "+ $f$ is the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff262f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The inputs weights, biases, and outputs in the above expression can be written in matrix form as:\n",
    "\n",
    "$$\\mathbf{x} = \\left[\t\n",
    "\t\\begin{array}{c }\n",
    "    \tx_1   \\\\\n",
    "     \tx_2   \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n  \\end{array}\\right], \\mathbf{W} = \\left[\t\n",
    "\t\\begin{array}{c c c }\n",
    "    \tw_{11} & w_{12} & \\dots  & w_{1m} \\\\\n",
    "     \tw_{21} & w_{22} & \\dots  & w_{2m} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w_{n1} & w_{n2} & \\dots  & w_{nm}\\end{array}\\right]$$\n",
    "        \n",
    "$$\\mathbf{b} = \\left[\t\n",
    "\t\\begin{array}{c }\n",
    "    \tb_1   \\\\\n",
    "     \tb_2   \\\\\n",
    "        \\vdots \\\\\n",
    "        b_m  \\end{array}\\right], \\mathbf{y} = \\left[\t\n",
    "\t\\begin{array}{c }\n",
    "    \ty_1   \\\\\n",
    "     \ty_2   \\\\\n",
    "        \\vdots \\\\\n",
    "        y_m  \\end{array}\\right]$$\n",
    "\n",
    "Then, the mathematical expression describing the SLP can be re-written as:\n",
    "\n",
    "$$ \\mathbf{y}^T = f\\left( \\mathbf{x}^T \\mathbf{W} + \\mathbf{b}^T \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf4ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "As an example, consider a set of points in the 2D plane, stored in the accompanying `plane_points_3_classes.txt` and `plane_points_3_classes_labels.txt` files. They points are divided into three classes and can be loaded and visualised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983ccb20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np #import numpy\n",
    "from matplotlib import pyplot as plt #import pyplot\n",
    "\n",
    "#load points and labels\n",
    "x3 = np.loadtxt('plane_points_3_classes.txt')\n",
    "labels3 = np.loadtxt('plane_points_3_classes_labels.txt')\n",
    "\n",
    "#use np.argwhere to find the indices of the points belonging to each class\n",
    "class0 = np.argwhere(labels3==0)\n",
    "class1 = np.argwhere(labels3==1)\n",
    "class2 = np.argwhere(labels3==2)\n",
    "\n",
    "#plot the points belonging to each class using a different color\n",
    "plt.plot(x3[class0,0],x3[class0,1],'.r')\n",
    "plt.plot(x3[class1,0],x3[class1,1],'.k')\n",
    "plt.plot(x3[class2,0],x3[class2,1],'.b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab9e43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Training with tensorflow\n",
    "\n",
    "Next a SLP with three outputs and a sofmax activation function can be trained to classify the points using `keras`. The syntax is almost identical to the one used in the previous lecture, with three differences:\n",
    "+ Three outputs are used to match the number of classes\n",
    "+ The `activation` option is set to `softmax`, to convert the outputs to probabilities\n",
    "+ `SparseCategoricalEntropy` is used as a loss. This option is suitable for classification with multiple classes, where the labels are inegers corresponding to the class number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa322245",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "32/32 [==============================] - 1s 2ms/step - loss: 1.2647 - accuracy: 0.2890\n",
      "Epoch 2/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2356 - accuracy: 0.3110\n",
      "Epoch 3/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2078 - accuracy: 0.3330\n",
      "Epoch 4/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1808 - accuracy: 0.3590\n",
      "Epoch 5/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1547 - accuracy: 0.3910\n",
      "Epoch 6/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1299 - accuracy: 0.4200\n",
      "Epoch 7/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1058 - accuracy: 0.4450\n",
      "Epoch 8/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0825 - accuracy: 0.4730\n",
      "Epoch 9/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0600 - accuracy: 0.5000\n",
      "Epoch 10/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0382 - accuracy: 0.5280\n",
      "Epoch 11/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0173 - accuracy: 0.5640\n",
      "Epoch 12/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9971 - accuracy: 0.5990\n",
      "Epoch 13/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9772 - accuracy: 0.6650\n",
      "Epoch 14/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9582 - accuracy: 0.7130\n",
      "Epoch 15/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9396 - accuracy: 0.7170\n",
      "Epoch 16/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9216 - accuracy: 0.7170\n",
      "Epoch 17/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9043 - accuracy: 0.7170\n",
      "Epoch 18/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8875 - accuracy: 0.7170\n",
      "Epoch 19/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8712 - accuracy: 0.7170\n",
      "Epoch 20/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8555 - accuracy: 0.7170\n",
      "Epoch 21/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8403 - accuracy: 0.7170\n",
      "Epoch 22/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8255 - accuracy: 0.7160\n",
      "Epoch 23/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.8111 - accuracy: 0.7180\n",
      "Epoch 24/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7973 - accuracy: 0.7230\n",
      "Epoch 25/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7837 - accuracy: 0.7280\n",
      "Epoch 26/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7707 - accuracy: 0.7420\n",
      "Epoch 27/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7581 - accuracy: 0.7540\n",
      "Epoch 28/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7457 - accuracy: 0.7680\n",
      "Epoch 29/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7337 - accuracy: 0.7760\n",
      "Epoch 30/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7222 - accuracy: 0.7920\n",
      "Epoch 31/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7109 - accuracy: 0.8020\n",
      "Epoch 32/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7000 - accuracy: 0.8150\n",
      "Epoch 33/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.8290\n",
      "Epoch 34/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6790 - accuracy: 0.8390\n",
      "Epoch 35/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.8490\n",
      "Epoch 36/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.8520\n",
      "Epoch 37/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.8630\n",
      "Epoch 38/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.8690\n",
      "Epoch 39/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6319 - accuracy: 0.8750\n",
      "Epoch 40/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6233 - accuracy: 0.8890\n",
      "Epoch 41/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.8940\n",
      "Epoch 42/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.9000\n",
      "Epoch 43/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5989 - accuracy: 0.9030\n",
      "Epoch 44/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.9100\n",
      "Epoch 45/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.9170\n",
      "Epoch 46/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.9210\n",
      "Epoch 47/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.9260\n",
      "Epoch 48/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5623 - accuracy: 0.9260\n",
      "Epoch 49/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5555 - accuracy: 0.9310\n",
      "Epoch 50/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.9320\n",
      "Epoch 51/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.9340\n",
      "Epoch 52/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.9360\n",
      "Epoch 53/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.9360\n",
      "Epoch 54/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.9400\n",
      "Epoch 55/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.9420\n",
      "Epoch 56/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.9460\n",
      "Epoch 57/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5072 - accuracy: 0.9510\n",
      "Epoch 58/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.9530\n",
      "Epoch 59/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.9550\n",
      "Epoch 60/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.9540\n",
      "Epoch 61/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.9560\n",
      "Epoch 62/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.9590\n",
      "Epoch 63/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.9600\n",
      "Epoch 64/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.9620\n",
      "Epoch 65/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.9600\n",
      "Epoch 66/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.9610\n",
      "Epoch 67/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.9610\n",
      "Epoch 68/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.9610\n",
      "Epoch 69/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.9620\n",
      "Epoch 70/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.9630\n",
      "Epoch 71/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.9640\n",
      "Epoch 72/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.9660\n",
      "Epoch 73/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.9670\n",
      "Epoch 74/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.9670\n",
      "Epoch 75/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.9660\n",
      "Epoch 76/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.9660\n",
      "Epoch 77/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.9660\n",
      "Epoch 78/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4149 - accuracy: 0.9660\n",
      "Epoch 79/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4114 - accuracy: 0.9660\n",
      "Epoch 80/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4081 - accuracy: 0.9650\n",
      "Epoch 81/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.9660\n",
      "Epoch 82/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.9660\n",
      "Epoch 83/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.9670\n",
      "Epoch 84/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.9670\n",
      "Epoch 85/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.9680\n",
      "Epoch 86/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.9690\n",
      "Epoch 87/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.9690\n",
      "Epoch 88/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.9690\n",
      "Epoch 89/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.9710\n",
      "Epoch 90/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.9710\n",
      "Epoch 91/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3742 - accuracy: 0.9700\n",
      "Epoch 92/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.9700\n",
      "Epoch 93/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.9690\n",
      "Epoch 94/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3659 - accuracy: 0.9690\n",
      "Epoch 95/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.9690\n",
      "Epoch 96/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3607 - accuracy: 0.9690\n",
      "Epoch 97/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.9700\n",
      "Epoch 98/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.9690\n",
      "Epoch 99/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.9690\n",
      "Epoch 100/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.9680\n",
      "Epoch 101/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.9680\n",
      "Epoch 102/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3459 - accuracy: 0.9680\n",
      "Epoch 103/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.9690\n",
      "Epoch 104/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.9680\n",
      "Epoch 105/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.9680\n",
      "Epoch 106/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.9680\n",
      "Epoch 107/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.9670\n",
      "Epoch 108/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.9670\n",
      "Epoch 109/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.9670\n",
      "Epoch 110/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.9660\n",
      "Epoch 111/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3260 - accuracy: 0.9670\n",
      "Epoch 112/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.9660\n",
      "Epoch 113/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.9640\n",
      "Epoch 114/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.9670\n",
      "Epoch 115/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.9650\n",
      "Epoch 116/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.9640\n",
      "Epoch 117/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.9660\n",
      "Epoch 118/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3122 - accuracy: 0.9650\n",
      "Epoch 119/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.9660\n",
      "Epoch 120/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.9670\n",
      "Epoch 121/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.9650\n",
      "Epoch 122/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3049 - accuracy: 0.9640\n",
      "Epoch 123/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.9650\n",
      "Epoch 124/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.9670\n",
      "Epoch 125/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2996 - accuracy: 0.9670\n",
      "Epoch 126/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2979 - accuracy: 0.9670\n",
      "Epoch 127/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.9670\n",
      "Epoch 128/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.9640\n",
      "Epoch 129/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.9650\n",
      "Epoch 130/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.9650\n",
      "Epoch 131/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.9660\n",
      "Epoch 132/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.9670\n",
      "Epoch 133/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.9670\n",
      "Epoch 134/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.9650\n",
      "Epoch 135/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2834 - accuracy: 0.9670\n",
      "Epoch 136/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.9670\n",
      "Epoch 137/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9680\n",
      "Epoch 138/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.9670\n",
      "Epoch 139/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.9650\n",
      "Epoch 140/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.9670\n",
      "Epoch 141/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.9670\n",
      "Epoch 142/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.9670\n",
      "Epoch 143/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.9680\n",
      "Epoch 144/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.9670\n",
      "Epoch 145/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2690 - accuracy: 0.9660\n",
      "Epoch 146/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.9660\n",
      "Epoch 147/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.9660\n",
      "Epoch 148/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.9670\n",
      "Epoch 149/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.9650\n",
      "Epoch 150/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.9650\n",
      "Epoch 151/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.9660\n",
      "Epoch 152/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.9660\n",
      "Epoch 153/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2585 - accuracy: 0.9660\n",
      "Epoch 154/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2573 - accuracy: 0.9660\n",
      "Epoch 155/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.9670\n",
      "Epoch 156/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.9660\n",
      "Epoch 157/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2536 - accuracy: 0.9670\n",
      "Epoch 158/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.9680\n",
      "Epoch 159/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2513 - accuracy: 0.9690\n",
      "Epoch 160/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.9690\n",
      "Epoch 161/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9680\n",
      "Epoch 162/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9680\n",
      "Epoch 163/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.9690\n",
      "Epoch 164/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9690\n",
      "Epoch 165/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2445 - accuracy: 0.9690\n",
      "Epoch 166/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2433 - accuracy: 0.9700\n",
      "Epoch 167/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9700\n",
      "Epoch 168/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.9690\n",
      "Epoch 169/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.9690\n",
      "Epoch 170/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9700\n",
      "Epoch 171/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2380 - accuracy: 0.9690\n",
      "Epoch 172/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.9700\n",
      "Epoch 173/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.9700\n",
      "Epoch 174/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.9700\n",
      "Epoch 175/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2339 - accuracy: 0.9700\n",
      "Epoch 176/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.9690\n",
      "Epoch 177/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.9700\n",
      "Epoch 178/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2309 - accuracy: 0.9700\n",
      "Epoch 179/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9700\n",
      "Epoch 180/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.9700\n",
      "Epoch 181/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.9700\n",
      "Epoch 182/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9700\n",
      "Epoch 183/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9700\n",
      "Epoch 184/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2252 - accuracy: 0.9700\n",
      "Epoch 185/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9700\n",
      "Epoch 186/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.9700\n",
      "Epoch 187/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2225 - accuracy: 0.9700\n",
      "Epoch 188/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2216 - accuracy: 0.9690\n",
      "Epoch 189/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.9700\n",
      "Epoch 190/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2198 - accuracy: 0.9710\n",
      "Epoch 191/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2190 - accuracy: 0.9700\n",
      "Epoch 192/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2181 - accuracy: 0.9700\n",
      "Epoch 193/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9700\n",
      "Epoch 194/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2164 - accuracy: 0.9700\n",
      "Epoch 195/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2155 - accuracy: 0.9700\n",
      "Epoch 196/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2147 - accuracy: 0.9710\n",
      "Epoch 197/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9700\n",
      "Epoch 198/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9700\n",
      "Epoch 199/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.9700\n",
      "Epoch 200/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2114 - accuracy: 0.9700\n",
      "Epoch 201/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9700\n",
      "Epoch 202/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9700\n",
      "Epoch 203/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2090 - accuracy: 0.9710\n",
      "Epoch 204/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.9720\n",
      "Epoch 205/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.9720\n",
      "Epoch 206/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9720\n",
      "Epoch 207/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9730\n",
      "Epoch 208/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9730\n",
      "Epoch 209/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2044 - accuracy: 0.9730\n",
      "Epoch 210/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9730\n",
      "Epoch 211/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2029 - accuracy: 0.9730\n",
      "Epoch 212/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9740\n",
      "Epoch 213/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9740\n",
      "Epoch 214/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9730\n",
      "Epoch 215/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2000 - accuracy: 0.9730\n",
      "Epoch 216/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1993 - accuracy: 0.9730\n",
      "Epoch 217/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1986 - accuracy: 0.9730\n",
      "Epoch 218/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1980 - accuracy: 0.9740\n",
      "Epoch 219/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9740\n",
      "Epoch 220/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9740\n",
      "Epoch 221/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1959 - accuracy: 0.9740\n",
      "Epoch 222/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.9740\n",
      "Epoch 223/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1946 - accuracy: 0.9750\n",
      "Epoch 224/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1939 - accuracy: 0.9740\n",
      "Epoch 225/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1932 - accuracy: 0.9740\n",
      "Epoch 226/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1926 - accuracy: 0.9740\n",
      "Epoch 227/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9760\n",
      "Epoch 228/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9770\n",
      "Epoch 229/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9760\n",
      "Epoch 230/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.9770\n",
      "Epoch 231/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.9770\n",
      "Epoch 232/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1888 - accuracy: 0.9770\n",
      "Epoch 233/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1882 - accuracy: 0.9760\n",
      "Epoch 234/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9780\n",
      "Epoch 235/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1870 - accuracy: 0.9770\n",
      "Epoch 236/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1863 - accuracy: 0.9770\n",
      "Epoch 237/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9770\n",
      "Epoch 238/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1852 - accuracy: 0.9760\n",
      "Epoch 239/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1846 - accuracy: 0.9780\n",
      "Epoch 240/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1840 - accuracy: 0.9770\n",
      "Epoch 241/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.9770\n",
      "Epoch 242/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1828 - accuracy: 0.9760\n",
      "Epoch 243/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1822 - accuracy: 0.9780\n",
      "Epoch 244/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1817 - accuracy: 0.9780\n",
      "Epoch 245/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1811 - accuracy: 0.9760\n",
      "Epoch 246/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1805 - accuracy: 0.9760\n",
      "Epoch 247/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1800 - accuracy: 0.9770\n",
      "Epoch 248/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1794 - accuracy: 0.9780\n",
      "Epoch 249/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1789 - accuracy: 0.9780\n",
      "Epoch 250/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1784 - accuracy: 0.9780\n",
      "Epoch 251/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1779 - accuracy: 0.9780\n",
      "Epoch 252/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1773 - accuracy: 0.9780\n",
      "Epoch 253/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1768 - accuracy: 0.9780\n",
      "Epoch 254/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1763 - accuracy: 0.9790\n",
      "Epoch 255/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1758 - accuracy: 0.9780\n",
      "Epoch 256/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1752 - accuracy: 0.9780\n",
      "Epoch 257/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1748 - accuracy: 0.9780\n",
      "Epoch 258/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9780\n",
      "Epoch 259/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1737 - accuracy: 0.9780\n",
      "Epoch 260/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.9780\n",
      "Epoch 261/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1727 - accuracy: 0.9780\n",
      "Epoch 262/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1722 - accuracy: 0.9790\n",
      "Epoch 263/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1717 - accuracy: 0.9790\n",
      "Epoch 264/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1712 - accuracy: 0.9790\n",
      "Epoch 265/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9790\n",
      "Epoch 266/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1703 - accuracy: 0.9790\n",
      "Epoch 267/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9790\n",
      "Epoch 268/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9790\n",
      "Epoch 269/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1689 - accuracy: 0.9790\n",
      "Epoch 270/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1684 - accuracy: 0.9790\n",
      "Epoch 271/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9790\n",
      "Epoch 272/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9790\n",
      "Epoch 273/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.9790\n",
      "Epoch 274/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9790\n",
      "Epoch 275/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1662 - accuracy: 0.9790\n",
      "Epoch 276/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1658 - accuracy: 0.9790\n",
      "Epoch 277/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9790\n",
      "Epoch 278/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1648 - accuracy: 0.9800\n",
      "Epoch 279/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1644 - accuracy: 0.9800\n",
      "Epoch 280/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1639 - accuracy: 0.9800\n",
      "Epoch 281/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1635 - accuracy: 0.9800\n",
      "Epoch 282/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1631 - accuracy: 0.9790\n",
      "Epoch 283/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1627 - accuracy: 0.9800\n",
      "Epoch 284/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1622 - accuracy: 0.9810\n",
      "Epoch 285/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1618 - accuracy: 0.9810\n",
      "Epoch 286/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1614 - accuracy: 0.9800\n",
      "Epoch 287/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.9810\n",
      "Epoch 288/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.9810\n",
      "Epoch 289/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1601 - accuracy: 0.9810\n",
      "Epoch 290/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1598 - accuracy: 0.9800\n",
      "Epoch 291/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9810\n",
      "Epoch 292/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.9800\n",
      "Epoch 293/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1586 - accuracy: 0.9820\n",
      "Epoch 294/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9820\n",
      "Epoch 295/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1578 - accuracy: 0.9810\n",
      "Epoch 296/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1574 - accuracy: 0.9810\n",
      "Epoch 297/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1570 - accuracy: 0.9810\n",
      "Epoch 298/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.9810\n",
      "Epoch 299/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1563 - accuracy: 0.9820\n",
      "Epoch 300/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1559 - accuracy: 0.9810\n",
      "Epoch 301/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1555 - accuracy: 0.9810\n",
      "Epoch 302/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1551 - accuracy: 0.9810\n",
      "Epoch 303/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1547 - accuracy: 0.9820\n",
      "Epoch 304/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 0.9820\n",
      "Epoch 305/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9810\n",
      "Epoch 306/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1537 - accuracy: 0.9820\n",
      "Epoch 307/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9820\n",
      "Epoch 308/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1529 - accuracy: 0.9820\n",
      "Epoch 309/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9820\n",
      "Epoch 310/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9820\n",
      "Epoch 311/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9820\n",
      "Epoch 312/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1515 - accuracy: 0.9820\n",
      "Epoch 313/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9820\n",
      "Epoch 314/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.9820\n",
      "Epoch 315/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9820\n",
      "Epoch 316/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9820\n",
      "Epoch 317/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.9820\n",
      "Epoch 318/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1494 - accuracy: 0.9820\n",
      "Epoch 319/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.9820\n",
      "Epoch 320/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9820\n",
      "Epoch 321/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.9820\n",
      "Epoch 322/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9820\n",
      "Epoch 323/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1477 - accuracy: 0.9820\n",
      "Epoch 324/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1474 - accuracy: 0.9820\n",
      "Epoch 325/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9820\n",
      "Epoch 326/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9820\n",
      "Epoch 327/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9820\n",
      "Epoch 328/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9820\n",
      "Epoch 329/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1458 - accuracy: 0.9820\n",
      "Epoch 330/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9820\n",
      "Epoch 331/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1452 - accuracy: 0.9820\n",
      "Epoch 332/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9820\n",
      "Epoch 333/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.9820\n",
      "Epoch 334/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1443 - accuracy: 0.9820\n",
      "Epoch 335/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9820\n",
      "Epoch 336/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9820\n",
      "Epoch 337/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.9820\n",
      "Epoch 338/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9820\n",
      "Epoch 339/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1428 - accuracy: 0.9820\n",
      "Epoch 340/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9820\n",
      "Epoch 341/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9820\n",
      "Epoch 342/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9820\n",
      "Epoch 343/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9820\n",
      "Epoch 344/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9820\n",
      "Epoch 345/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1410 - accuracy: 0.9820\n",
      "Epoch 346/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9820\n",
      "Epoch 347/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1405 - accuracy: 0.9820\n",
      "Epoch 348/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9820\n",
      "Epoch 349/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1399 - accuracy: 0.9820\n",
      "Epoch 350/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1396 - accuracy: 0.9820\n",
      "Epoch 351/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.9820\n",
      "Epoch 352/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9820\n",
      "Epoch 353/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9820\n",
      "Epoch 354/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9820\n",
      "Epoch 355/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1383 - accuracy: 0.9820\n",
      "Epoch 356/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1381 - accuracy: 0.9820\n",
      "Epoch 357/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9820\n",
      "Epoch 358/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.9820\n",
      "Epoch 359/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1372 - accuracy: 0.9820\n",
      "Epoch 360/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9820\n",
      "Epoch 361/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9820\n",
      "Epoch 362/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1365 - accuracy: 0.9820\n",
      "Epoch 363/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1362 - accuracy: 0.9820\n",
      "Epoch 364/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1360 - accuracy: 0.9820\n",
      "Epoch 365/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1357 - accuracy: 0.9820\n",
      "Epoch 366/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1355 - accuracy: 0.9820\n",
      "Epoch 367/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9820\n",
      "Epoch 368/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1349 - accuracy: 0.9820\n",
      "Epoch 369/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1347 - accuracy: 0.9820\n",
      "Epoch 370/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1345 - accuracy: 0.9820\n",
      "Epoch 371/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1342 - accuracy: 0.9820\n",
      "Epoch 372/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9820\n",
      "Epoch 373/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9820\n",
      "Epoch 374/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9820\n",
      "Epoch 375/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9820\n",
      "Epoch 376/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9820\n",
      "Epoch 377/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9820\n",
      "Epoch 378/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9820\n",
      "Epoch 379/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9820\n",
      "Epoch 380/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9820\n",
      "Epoch 381/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9820\n",
      "Epoch 382/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1316 - accuracy: 0.9820\n",
      "Epoch 383/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1313 - accuracy: 0.9820\n",
      "Epoch 384/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9820\n",
      "Epoch 385/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9820\n",
      "Epoch 386/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9820\n",
      "Epoch 387/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9820\n",
      "Epoch 388/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1302 - accuracy: 0.9820\n",
      "Epoch 389/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9820\n",
      "Epoch 390/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1298 - accuracy: 0.9830\n",
      "Epoch 391/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9840\n",
      "Epoch 392/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1294 - accuracy: 0.9820\n",
      "Epoch 393/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9820\n",
      "Epoch 394/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9820\n",
      "Epoch 395/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9820\n",
      "Epoch 396/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9820\n",
      "Epoch 397/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9830\n",
      "Epoch 398/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9830\n",
      "Epoch 399/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1278 - accuracy: 0.9830\n",
      "Epoch 400/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9820\n",
      "Epoch 401/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9820\n",
      "Epoch 402/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9820\n",
      "Epoch 403/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9820\n",
      "Epoch 404/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9820\n",
      "Epoch 405/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9820\n",
      "Epoch 406/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9830\n",
      "Epoch 407/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9830\n",
      "Epoch 408/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1260 - accuracy: 0.9830\n",
      "Epoch 409/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9840\n",
      "Epoch 410/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9830\n",
      "Epoch 411/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9830\n",
      "Epoch 412/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9830\n",
      "Epoch 413/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9830\n",
      "Epoch 414/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9830\n",
      "Epoch 415/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9830\n",
      "Epoch 416/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1244 - accuracy: 0.9830\n",
      "Epoch 417/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1242 - accuracy: 0.9840\n",
      "Epoch 418/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.9840\n",
      "Epoch 419/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1238 - accuracy: 0.9840\n",
      "Epoch 420/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.9840\n",
      "Epoch 421/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9840\n",
      "Epoch 422/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1233 - accuracy: 0.9840\n",
      "Epoch 423/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9840\n",
      "Epoch 424/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9840\n",
      "Epoch 425/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1227 - accuracy: 0.9840\n",
      "Epoch 426/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9840\n",
      "Epoch 427/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.9840\n",
      "Epoch 428/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9840\n",
      "Epoch 429/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9840\n",
      "Epoch 430/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9840\n",
      "Epoch 431/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1216 - accuracy: 0.9830\n",
      "Epoch 432/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9830\n",
      "Epoch 433/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9840\n",
      "Epoch 434/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9840\n",
      "Epoch 435/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9840\n",
      "Epoch 436/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1207 - accuracy: 0.9840\n",
      "Epoch 437/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1205 - accuracy: 0.9840\n",
      "Epoch 438/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1204 - accuracy: 0.9840\n",
      "Epoch 439/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.9840\n",
      "Epoch 440/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.9840\n",
      "Epoch 441/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1199 - accuracy: 0.9830\n",
      "Epoch 442/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1197 - accuracy: 0.9840\n",
      "Epoch 443/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9840\n",
      "Epoch 444/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9840\n",
      "Epoch 445/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9840\n",
      "Epoch 446/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 0.9840\n",
      "Epoch 447/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1188 - accuracy: 0.9840\n",
      "Epoch 448/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9840\n",
      "Epoch 449/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1185 - accuracy: 0.9840\n",
      "Epoch 450/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1183 - accuracy: 0.9840\n",
      "Epoch 451/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1182 - accuracy: 0.9840\n",
      "Epoch 452/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1180 - accuracy: 0.9840\n",
      "Epoch 453/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9840\n",
      "Epoch 454/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9840\n",
      "Epoch 455/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9840\n",
      "Epoch 456/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9840\n",
      "Epoch 457/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9840\n",
      "Epoch 458/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1170 - accuracy: 0.9840\n",
      "Epoch 459/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1169 - accuracy: 0.9840\n",
      "Epoch 460/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9840\n",
      "Epoch 461/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9840\n",
      "Epoch 462/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9840\n",
      "Epoch 463/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9840\n",
      "Epoch 464/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.9840\n",
      "Epoch 465/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9840\n",
      "Epoch 466/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9840\n",
      "Epoch 467/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.9840\n",
      "Epoch 468/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9840\n",
      "Epoch 469/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.9840\n",
      "Epoch 470/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9840\n",
      "Epoch 471/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.9840\n",
      "Epoch 472/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9840\n",
      "Epoch 473/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1147 - accuracy: 0.9840\n",
      "Epoch 474/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.9840\n",
      "Epoch 475/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1144 - accuracy: 0.9840\n",
      "Epoch 476/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1142 - accuracy: 0.9840\n",
      "Epoch 477/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1141 - accuracy: 0.9850\n",
      "Epoch 478/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1139 - accuracy: 0.9850\n",
      "Epoch 479/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1138 - accuracy: 0.9850\n",
      "Epoch 480/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.9850\n",
      "Epoch 481/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1135 - accuracy: 0.9850\n",
      "Epoch 482/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1133 - accuracy: 0.9850\n",
      "Epoch 483/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.9850\n",
      "Epoch 484/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9850\n",
      "Epoch 485/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.9840\n",
      "Epoch 486/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.9850\n",
      "Epoch 487/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 0.9850\n",
      "Epoch 488/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.9850\n",
      "Epoch 489/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1123 - accuracy: 0.9850\n",
      "Epoch 490/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1122 - accuracy: 0.9850\n",
      "Epoch 491/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9850\n",
      "Epoch 492/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1119 - accuracy: 0.9850\n",
      "Epoch 493/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9850\n",
      "Epoch 494/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9850\n",
      "Epoch 495/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1115 - accuracy: 0.9850\n",
      "Epoch 496/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9850\n",
      "Epoch 497/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.9850\n",
      "Epoch 498/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9850\n",
      "Epoch 499/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9850\n",
      "Epoch 500/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aa3f68ab90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf #import tensorflow\n",
    "\n",
    "modelSLP3 = tf.keras.Sequential() #create sequential model\n",
    "\n",
    "#add layer with 3 outputs and softmax activation\n",
    "modelSLP3.add(tf.keras.layers.Dense(3,activation='softmax'))\n",
    "\n",
    "#compile model, SparseCategoricalCrossentropy is used as a loss\n",
    "modelSLP3.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='accuracy')\n",
    "\n",
    "#train the model for 100 epochs using the training data\n",
    "modelSLP3.fit(x3, labels3, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024856e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "Results can be visualised through a confusion matrix, using the same commands as shown last week. Notice that the `argmax` function is used to convert predicted probabilities to class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69dc176",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2aa3fcff070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay #import confusion matrix related classes\n",
    "\n",
    "#use model to predict the labels of the training data as probabilities and convert to integers\n",
    "labelsPred = modelSLP3.predict(x3)\n",
    "labelsPred = np.argmax(labelsPred,axis=1)\n",
    "\n",
    "\n",
    "#create confusion matrix\n",
    "confusionMatrix3=confusion_matrix(labels3,labelsPred)\n",
    "\n",
    "#create and show confusion matrix plot\n",
    "confusionMatrixPlot3 = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix3)\n",
    "confusionMatrixPlot3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325f86c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visualising the decision boundary\n",
    "\n",
    "To better understand the properties of the created models, it can be helpful to visualise the boundaries between classes, also termed 'decision boundaries'. To this end we will use the following function, which is a slightly modified version of the one provided in [Jon Charests'notebook](https://jonchar.net/notebooks/Artificial-Neural-Network-with-Keras/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6e1279",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(model,limx=[-1,1],limy=[-1,1],resolution=200, colormap = 'RdBu'):\n",
    "    '''Function to plot the decision boundary of a tensorflow model as a contour plot.\n",
    "       model is the model, which should have a predict method\n",
    "       limx, limy are the limits of the plot in x and y\n",
    "       resolution is the resolution of the plot in terms of the number of points used per direction\n",
    "       colormap is the colormap to be used for the contour plot\n",
    "       The function returns a figure and an axis object'''\n",
    "    \n",
    "    #create figure\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    \n",
    "    #create linspaces with the x and y coordinates of the points to be used for the contour plot\n",
    "    #the limits and resolution are set to the user provided values\n",
    "    xPoints = np.linspace(limx[0], limx[1], resolution)\n",
    "    yPoints = np.linspace(limy[0], limy[1], resolution)\n",
    "    \n",
    "    #create a meshgrid from the provided\n",
    "    xx, yy = np.meshgrid(xPoints, yPoints)\n",
    "    \n",
    "    #use model to predict the labels of the generated points\n",
    "    #ravel and c_ are used to bring the coordinates in the correct shape\n",
    "    modelPred = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    #check shape of the output, if the output is twodimensional and the size in the second dimension is more than 1\n",
    "    #then the ouput is assumed to be provided in terms of probabilities and is converted to class indices using the argmax function\n",
    "    if len(modelPred.shape)==2 and modelPred.shape[1]!=1:\n",
    "        modelPred = np.argmax(modelPred,axis=1)\n",
    "    else:\n",
    "    #if the output is either onedimensional or the second dimension has size one, \n",
    "    #then it is assumed to be labels for binary classification and a 0.5 threshold is applied to convert to binary format\n",
    "        modelPred = modelPred>0.5\n",
    "    \n",
    "    #reshape the labels to the shape of xx and yy so that they can be used for a contour plot\n",
    "    z = modelPred.reshape(xx.shape)\n",
    "\n",
    "    #create contour plot\n",
    "    ax.contourf(xx,yy,z,cmap=colormap,alpha=0.5)\n",
    "    \n",
    "    #return figure and axis\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d2ace",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The function can be used to plot the decision boundary of the model, along with the points corresponding to the different classes as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad33dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aa42543a00>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#call provided function, with modelSLP3 as input\n",
    "#this creates a contour plot with different colors used for each class\n",
    "fig, ax = plotDecisionBoundary(modelSLP3)\n",
    "\n",
    "#plot points of each class using a different color\n",
    "ax.plot(x3[class0,0],x3[class0,1],'.r')\n",
    "ax.plot(x3[class1,0],x3[class1,1],'.k')\n",
    "ax.plot(x3[class2,0],x3[class2,1],'.b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2ae9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Multi Layer Perceptron\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The above can be further extended to create more complex architectures termed Multi Layer Perceptrons (MLPs) or feedforward neural networks. These consist of multiple layers, each similar to a SLP, placed one after the other, with the outputs of each previous layer serving as an input for the next. All layers apart from the input and output ones are termed hidden layers. In the following figure, an example of a simple such architecture, consisting of one hidden layer, in addition to the input and output layer, is illustrated:\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/mlp1.png\" width=\"600\" align=\"center\">\n",
    "</div>\n",
    "\n",
    "Next the individual elements of a MLP are described in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dd8a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inputs\n",
    "\n",
    "The multi-dimensional inputs $x_1, x_2, \\dots, x_n$ of the MLP are similar to the ones of the SLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff6b4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hidden layers\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/mlp_hidden_layers.png\" width=\"1000\" align=\"center\">\n",
    "    <br/><a align=\"center\">MLP with 1, 2 and 3 hidden layers respectively</a>\n",
    "</div>\n",
    "\n",
    "All layers apart from the input and output ones are termed 'hidden'. Each of the hidden layers includes a number of units, each with a set of weights and biases, as well as an activation function. As will be also mentioned later, networks with at least one hidden layer can represent nonlinear functions for regression, or nonlinear boundaries between classes for classification. However, the number of hidden layers used, can affect the properties of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd47e5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Hidden units\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/mlp_hidden_units.png\" width=\"1000\" align=\"center\">\n",
    "    <br/><a align=\"center\">MLP with 1 hidden layer and 3, 5, and 7 hidden units respectively</a>\n",
    "</div>\n",
    "\n",
    "Each hidden layer consists of a number of units, connected to all outputs of the previous layer and acting as inputs for the next layer. The number of units in each hidden layer does not have to be equal to the number of inputs or outputs of the network, however it might affect the properties of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1290e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Weights, summing and bias\n",
    "\n",
    "In each layer of a MLP, the outputs of previous layers are used as input. Similar to the SLP, these inputs are weighted and summed at each unit and biases are added. The process is in general similar to the SLP case, however a larger number of weights and biases needs to be deterined as a result of the larger number of layers and units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882318f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Activation\n",
    "\n",
    "Similar to the SLP, activation is performed after weighting and summing at each layer. Some activation functions used in MLPs are given below:\n",
    "\n",
    "|     Activation function    |                Expression                    |                                 Graph                             |\n",
    "|----------------------------|----------------------------------------------|------------------------------------------------------------------|\n",
    "|Rectified Linear Unit (ReLU)|$$ f\\left( x \\right) = \\max\\left(x,0\\right) $$|<img src=\"./Figures/ReLU.png\" width=\"300\" align=\"center\"> |\n",
    "|Softsign                    |$$ f\\left( x \\right) = \\frac{x}{\\lvert x \\rvert+1} $$|<img src=\"./Figures/softsign.png\" width=\"300\" align=\"center\"> |\n",
    "|Softplus                    |$$ f\\left( x \\right) = \\log{\\left(e^{x}+1 \\right)} $$|<img src=\"./Figures/softplus.png\" width=\"300\" align=\"center\"> |\n",
    "\n",
    "In modern neural networks, the most popular of these is probably the Rectified Linear Unit (ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84685d3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Output layer\n",
    "\n",
    "In the output layer, weighting and summing is performed similar to hidden layers.\n",
    "\n",
    "##### Activation\n",
    "\n",
    "The activation function used for output layers, might be slightly different than hidden layers. For example:\n",
    "\n",
    "+ **For regression problems**, a linear activation might be used.\n",
    "+ **For classification problems**, a softmax activation function can be used to convert each output to a probability of the input belonging to a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f4381",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical expression\n",
    "\n",
    "In what follows, we will use superscritps in brackets to denote elements of the $i$th layer of a network, where $i$ assumes a value of 0 for the input layer, 1 for the first hidden layer etc. For instance:\n",
    "\n",
    "- $f^{\\left(2\\right)}$ denotes the activation function of the second hidden layer\n",
    "- $\\mathbf{W}^{\\left( 1 \\right)}$ denotes the matrix of weights of the first hidden layer\n",
    "\n",
    "Using this notation, the output of the first hidden layer of a MLP will be:\n",
    "\n",
    "$$\\left(\\mathbf{y}^{\\left( 1 \\right)}\\right)^T = f^{\\left(1 \\right)}\\left( \\mathbf{x}^T \\mathbf{W}^{\\left( 1 \\right)} + \\left(\\mathbf{b}^{\\left( 1 \\right)}\\right)^T \\right)$$\n",
    "\n",
    "This is subsequently used as input for the second layer, yielding the expression:\n",
    "\n",
    "$$ \\left(\\mathbf{y}^{\\left( 2 \\right)}\\right)^T = f^{\\left(2 \\right)}\\left( \\left(\\mathbf{y}^{\\left( 1 \\right)}\\right)^T \\mathbf{W}^{\\left( 2 \\right)} + \\left(\\mathbf{b}^{\\left( 2 \\right)}\\right)^T \\right) = f^{\\left(2 \\right)}\\left( \\left( f^{\\left(1 \\right)}\\left( \\mathbf{x}^T \\mathbf{W}^{\\left( 1 \\right)} + \\left(\\mathbf{b}^{\\left( 1 \\right)}\\right)^T \\right) \\right)^T \\mathbf{W}^{\\left( 2 \\right)} + \\left(\\mathbf{b}^{\\left( 2 \\right)}\\right)^T \\right)$$\n",
    "\n",
    "The process is then repeated for all layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3eb37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67719fcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss function\n",
    "\n",
    "As also mentioned in last week's lecture, different loss functions can be used to measure the discrepancy between predicted and actual labels. For classification problems, a commonly used loss, also employed in last week's computer lab, is cross entropy. For a binary classificartion problem, and a set of training labels $y^*_i$ and actual labels $y_i$, cross entropy can be defined as:\n",
    "\n",
    "$$E = -\\dfrac{1}{m} \\Sigma_{i=1}^m \\left[ y^*_i \\log{y_i} + \\left(1-y^*_i\\right) \\log{\\left( 1-y_i \\right) } \\right]$$\n",
    "\n",
    "where $m$ is the number of training data points. The definition can be extended to higher numebers of classes by considering a probability for each class, as mentioned above, and summing over the number of classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc09152",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back propagation\n",
    "\n",
    "Weights of MLPs can be updated using extensions of the methods described in the previous lecture for SLPs. An necessary step for most methods, is the computation of the partial derivatives of the loss function with respect to the weights and biases, which can be computed using the chain rule, for instance:\n",
    "\n",
    "$$\\dfrac{\\partial E}{\\partial w_{ij}} = \\Sigma_{k=1}^m\\dfrac{\\partial E}{\\partial y_{k}} \\dfrac{\\partial y_k}{\\partial w_{ij}}$$\n",
    "\n",
    "where $m$ is the number of training data points.\n",
    "\n",
    "Since MLPs involve more layers than SLPs the computation of $\\dfrac{\\partial y_k}{\\partial w_{ij}}$ is more involved, requiring the use of the chain rule multiple times. The process of using the chain rule recursively, is termed **back propagation** as oposed to **forward propagation**, which is the process of pasing inputs through the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cef7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "To illustrate the differences between SLPs and SLPc and demonstrate how SLPs can be trained with `Keras`, lets consider a simple example with points in the two dimensional plane. The coordinates and labels ofthe points are stored in the accompanying `plane_points_nal.txt` and `plane_points_nl_labels.txt`. The labels have been assigned based on whether the points are above or below the curve defined by:\n",
    "\n",
    "$$y = 8x^3-3x$$\n",
    "\n",
    "Thus, the decision boundary essentially corresponds to that curve.\n",
    "\n",
    "The points can be loaded and visualised along with the curve as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78120b06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#import numpy and pyplot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#load points and labels from files\n",
    "x=np.loadtxt('plane_points_nl.txt')\n",
    "labels = np.loadtxt('plane_points_nl_labels.txt')\n",
    "\n",
    "#use np.argwhere to find the indices of the points belonging to each class\n",
    "class0 = np.argwhere(labels==1)\n",
    "class1 = np.argwhere(labels==0)\n",
    "\n",
    "#Generate coordinates of points along the decision boundary\n",
    "xDB = np.linspace(-0.73,0.73,1000)\n",
    "yDB = 8*xDB**3-3*xDB\n",
    "\n",
    "#plot classes using different colors\n",
    "plt.plot(x[class0,0],x[class0,1],'.b')\n",
    "plt.plot(x[class1,0],x[class1,1],'.r')\n",
    "\n",
    "#plot decision boundary\n",
    "plt.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbbffae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast to the cases considered so far, the points are clearly not linearly separable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898ea4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification with a SLP\n",
    "\n",
    "As a first test, we can try to train a SLP for this problem. However, since the data is not linearly separable, we would expect it to perform poorly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf6335",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create a sequential model for the slp\n",
    "modelSLP = tf.keras.Sequential()\n",
    "\n",
    "#add a dense layer with 1 output and a sigmoid activation function\n",
    "modelSLP.add(tf.keras.layers.Dense(1,\n",
    "                                   activation = 'sigmoid'))\n",
    "\n",
    "#compile model\n",
    "modelSLP.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics='accuracy')\n",
    "\n",
    "#train model for 100 epochs using the data from the points \n",
    "modelSLP.fit(x, labels, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2e9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Results can be visualised in terms of the decision boundary as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecce93",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#plot decision boundary using provided function\n",
    "fig, ax = plotDecisionBoundary(modelSLP)\n",
    "\n",
    "#plot data points\n",
    "ax.plot(x[class0,0],x[class0,1],'.b')\n",
    "ax.plot(x[class1,0],x[class1,1],'.r')\n",
    "\n",
    "#plot actual decision boundary\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8eab9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We observe that:\n",
    "+ The SLP performs poorly in terms of the achieved accuracy, regardless of the number of training epochs used\n",
    "+ The decision boundary for the SLP is a linear approximation of the actual decision boundary, which however is not enough to provide accurate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a865c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training a MLP\n",
    "\n",
    "Next, let's try to train a MLP to improve the above results. Our network will consist of:\n",
    "\n",
    "+ One hidden layer with 8 hidden units and ReLU activation\n",
    "+ One output layer with 2 outputs and softmax activartion to convert outputs to probabilities\n",
    "\n",
    "Using `keras`, this can be accomplished in a very similar way to what was shown so far for the SLP with multiple outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320f44d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create a sequential model for the mlp\n",
    "modelMLP = tf.keras.Sequential()\n",
    "\n",
    "#add a dense layer with 8 outputs and a ReLU activation\n",
    "modelMLP.add(tf.keras.layers.Dense(8,\n",
    "                                   activation = 'ReLU'))\n",
    "\n",
    "#add a dense layer with 2 outputs and softmax activation to convert outputs to probabilities\n",
    "modelMLP.add(tf.keras.layers.Dense(2,\n",
    "                                   activation = 'softmax'))\n",
    "\n",
    "#compile and train model for 100 epochs  \n",
    "modelMLP.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics='accuracy')\n",
    "\n",
    "modelMLP.fit(x, labels, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbeeed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualising the decision boundary\n",
    "\n",
    "Results can be visualised in terms of the decision boundary as in the SLP case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfaf6ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#plot decision boundary using provided function\n",
    "fig, ax = plotDecisionBoundary(modelMLP)\n",
    "\n",
    "#plot data points\n",
    "ax.plot(x[class0,0],x[class0,1],'.b')\n",
    "ax.plot(x[class1,0],x[class1,1],'.r')\n",
    "\n",
    "#plot actual decision boundary\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8401edf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Based on the above it is clear that if a large enough number of hidden units and/or hidden layers are used, nonlinear decision boundaries can be approximated. In fact, it can be shown that MLPs with a single hidden layer can approximate almost any function with an arbitrary degree of accuracy, provided that enough hidden units are used. More generally, the ability of the model to represent different functions is termed **capacity**. For MLPs, the model's capacity is affected by parameters such as the number of hidden units and hidden layers, learning rate and number of training epochs.\n",
    "\n",
    "While the above is very important for the general applicability of neural networks, it does not provide any guidance on how to select the number of hidden units and/or hidden layers for a given problem. In practice, these parameters are determined through experimentation and/or experience. The next subsections introduce some important concepts that are important for the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55b12",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generalisation\n",
    "\n",
    "So far we have only tested the error introduced by SLPs/MLPs when reproducing the training data, i.e. the **training error**. However, the main goal of machine learning algorithms is to make accurate predictions for **previously unseen data**, a property called **generalisation**. The extent to which this goal is achieved can be quantified through the so called **generalisation error** or **test error**, i.e. the expected error for a new, unseen input. A simple way of estimating this error is by computing the error for a new dataset, independent from the training set, termed the **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37d216",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfitting/Overfitting\n",
    "\n",
    "Considering the above, the capacity of machine learning models should be chosen to satisy two requirements:\n",
    "\n",
    "1. **Small training error:** a high enough capacity should be used to represent the function/decision boundary for a given problem. If the capacity of the model is too low, it will lead to large values for both the training and test error, a phenomenon termed **underfitting**. As an example of underfitting, consider the case of the SLP used for the previous example. In that case, the capacity of the model was too low for the given problem, leading to poor results.\n",
    "2. **Small discrepancy between the training and test error:** the capacity of the model should not be too high, otherwise the model might 'memorise' every single data point in the data set, rather than learning the required function or decision boundary. This leads to a phenomenon called **overfitting**, where low training errors do not translate to low test/generalisation error. As will be shown in the next example, overfitting can lead to particularly poor results in the presence of noise.\n",
    "\n",
    "The typical relationship between model capacity and the different types of error is illustrated in the following figure: \n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/overfitting.png\" width=\"600\" align=\"center\">\n",
    "    <br/><a align=\"center\">Relationship between capacity and error. Figure inspired from (Goodfellow et al., 2016).</a>\n",
    "</div>\n",
    "\n",
    "A good training strategy, should, in general, aim at determining the optimal capacity for the model, i.e. the one that yields the smallest possible training error, without deteriorating the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abcf4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate how overfitting can lead to poor generalisation, we will consider a set of points similar to the one used in the previous example, but with added noise, which might also be present in real data. The data is included in the `plane_points_noise1.txt` and `plane_points_noise_labels1.txt` files and can be visualised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f73308",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xOF=np.loadtxt('plane_points_noise.txt')\n",
    "labelsOF = np.loadtxt('plane_points_noise_labels.txt')\n",
    "\n",
    "class0OF = np.argwhere(labelsOF==1)\n",
    "class1OF = np.argwhere(labelsOF==0)\n",
    "\n",
    "plt.plot(xOF[class0OF,0],xOF[class0OF,1],'.b')\n",
    "plt.plot(xOF[class1OF,0],xOF[class1OF,1],'.r')\n",
    "plt.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072084a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a first step, we will split the data into a training and test set. This can be done automatically, using the `train_test_split` function from the `sklearn` package. The function requires, apart from the initial datapoints and labels, the percentage of data to be used for the training set. As output, it returns two different sets of datapoints and labels that can be used for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762de181",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, labelsTrain, labelsTest = train_test_split(xOF,labelsOF, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c628e84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we will employ a model with very high capacity to reduce the training error as much as possible. To this end, we will create a model with 4 hidden layers consisting of 100 hidden units each and we will train for a very high number of epochs (1000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1a9f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "modelOF = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(2,\n",
    "                                  activation = 'softmax'))\n",
    "\n",
    "modelOF.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='accuracy')\n",
    "\n",
    "\n",
    "modelOF.fit(xTrain, \n",
    "            labelsTrain, \n",
    "            epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005666b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As seen above, a very good training error can be obtained, with accuracy in the training set exceeding 90%. However, this accuracy does not necessarily translate to good generalisation. To illustrate this, we can evaluate the loss and accuracy over the test set, using the evaluate method of the model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f526a15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "testLoss, testAcc = modelOF.evaluate(xTest,  labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeea67b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As seen above, the test accuracy is significantly lower than the training error. Some further insight can be obtained by plotting the decision boundary for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ceaf7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plotDecisionBoundary(modelOF)\n",
    "\n",
    "class0Train = np.argwhere(labelsTrain==1)\n",
    "class1Train = np.argwhere(labelsTrain==0)\n",
    "\n",
    "ax.plot(xTrain[class0Train,0],xTrain[class0Train,1],'.b')\n",
    "ax.plot(xTrain[class1Train,0],xTrain[class1Train,1],'.r')\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b7757",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As seen above, the MLP has essentially memorised almost every single point in the training set, including the noise, rather than learning the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c7d4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "In order to avoid overfitting, hyperparameters, such as the number of hidden layers, should be selected to minimise the generalisation error, rather than the training error. However, the test error cannot be directly used to guide the process since it might bias the model. To avoid this, a third data set is typically introduced, termed the **validation set**, based on which hyperparameters are selected.Then, hyperparameters can be determined either manually, or in an automated way, based on a process similar to the following:\n",
    "\n",
    "+ Divide available data into test, validation and training data sets.\n",
    "+ Define a set of possible values for all hyperparameters to be determined. For instance, possible values for the number of hidden units might be: $[10, 20, 50, 100, 200, 500, 1000]$.\n",
    "+ For each possible combination of parameters:\n",
    "    + Create and train a model.\n",
    "    + Evaluate the error of this model over the validation set.\n",
    "    + For the first model: save the model and resulting error as the optimal model\n",
    "    + For every subsequent model: if the error is smaller than the optimal model, replace the optimal with the current model.\n",
    "+ Once all potential combinations have been tested, evaluate the error of the optimal model over the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f91e48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate the above process, consider the dataset from the previous example. First we further split the training set into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97abda",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xVal, labelsTrain, labelsVal = train_test_split(xTrain,labelsTrain, train_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaa01e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we specify the sets of possible values for the model hyperparameters. In this case, we will only modify the number of hidden units for all hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ac958",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hiddenUnits = [2,5,10,20,50,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d3dd4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The sets of hidden units can be looped, with a model created and trained for each parameter value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82686f89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#loop list with possible numbers of hidden units\n",
    "for units in hiddenUnits:\n",
    "    #create model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    #add 4 layers with different numbers of hidden units\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "\n",
    "    #add final dense layer with softmax activation\n",
    "    model.add(tf.keras.layers.Dense(2,\n",
    "                                    activation = 'softmax'))\n",
    "\n",
    "    #compile and fit model for training set\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics='accuracy')\n",
    "\n",
    "    model.fit(xTrain, \n",
    "                labelsTrain, \n",
    "                epochs=1000)\n",
    "    \n",
    "    #evaluate model for validation set\n",
    "    lossVal, accVal = model.evaluate(xVal,  labelsVal)\n",
    "    \n",
    "    if units==hiddenUnits[0]:\n",
    "        #if this is the first model tested, save model, error, and accuracy\n",
    "        modelOpt=model\n",
    "        lossOpt = lossVal\n",
    "        accOpt = accVal\n",
    "        unitsOpt = units\n",
    "    else:\n",
    "        #for every subsequent model check accuracy, if higher than current optimal model then replace optimal\n",
    "        if accVal>accOpt:\n",
    "            modelOpt=model\n",
    "            lossOpt = lossVal\n",
    "            accOpt = accVal\n",
    "            unitsOpt = units\n",
    "\n",
    "#print details of optimal model\n",
    "print('Best loss over test set: ', lossOpt)\n",
    "print('Best accuracy over validation set: ', accOpt)\n",
    "print('Number of units selected: ', unitsOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ca84e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The selected number of hidden layers is much smaller than the original model! The performance of the optimal model over the test set can be evaluated next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546000e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lossTest, accTest = modelOpt.evaluate(xTest,  labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b7d11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model achieves a smaller discrepancy between training and test error.\n",
    "\n",
    "Also, the decision boundary for the new model can be plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef0938",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plotDecisionBoundary(modelOpt)\n",
    "\n",
    "#find indices of \n",
    "class0Train = np.argwhere(labelsTrain==1)\n",
    "class1Train = np.argwhere(labelsTrain==0)\n",
    "\n",
    "#plot data points, line used in first example, and line obtained after training\n",
    "ax.plot(xTrain[class0Train,0],xTrain[class0Train,1],'.b')\n",
    "ax.plot(xTrain[class1Train,0],xTrain[class1Train,1],'.r')\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d5655",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It can be clearly seen that, despite the noisy data, the optimal model leads to a much smoother decision boundary, closer to the initial one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdeb560",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularisation/Early stopping\n",
    "\n",
    "A variety of additional strategies can be employed to improve the generalisation error of MLPs and are collectively refered to as **regularisation** methods. In general, these methods introduce modifications, for instance in the loss function of training process, to improve the test error without necessarily altering the capacity of the model.\n",
    "\n",
    "While several such strategies are available, we will only describe one of the most commonly used for neural networks, named **early stopping**. The strategy is based on the observation that, during a typical training process, the training error will constantly decrease, while the validation error will follow a U shape, initially decreasing and increasing again after a certain point, as illustrated in the following figure:\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/training_validation_error.png\" width=\"600\" align=\"center\">\n",
    "    <br/><a align=\"center\">Training and validation error during training. Figure taken from (Goodfellow et al., 2016).</a>\n",
    "</div>\n",
    "\n",
    "Based on that, the process should ideally be stopped at the point where the minimum validation error is reached. This can be accomplished by monitoring the validation error during training and terminating the process once an increase is observed for a few consecutive epochs. Then the weights achieved at the minimum value can be restored leading to an optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff582c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate the effectiveness of early stopping, as well as how it can be implemented with `keras`, we will consider the previous example, with the same training, validation, testing set splits. The first modification required, is the definition of an early stopping callback. The following options are selected:\n",
    "\n",
    "+ The `monitor` parameter is set to `val_loss` to use the validation error as an early stopping criterion.\n",
    "+ The `patience` parameter is given a value of 10, meaning that if the error in the validation set is not improved after 10 consecutive epochs, training will stop.\n",
    "+ The `restore_best_weights` parameter is set to `True` to use the best weights achieved during training, rather than the last ones obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2dfe46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#define Early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299183c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to monitor the training and validation error, an additional class is imported from the [`livelossplot`](https://p.migdal.pl/livelossplot/#installation) module. The module does not come preinstalled with Anaconda, therefore it has to be installed by running the following command in an Anaconda (Windows) or regular (Mac) command line:\n",
    "\n",
    "```\n",
    "pip install livelossplot\n",
    "```\n",
    "\n",
    "Then, the required class, named `PlotLossesKeras` can be imported as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d7031",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0cfcc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model can be created and compiled in the same way as the one used to demonstrate overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecf138",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelES = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(1000,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(2,\n",
    "                                  activation = 'softmax'))\n",
    "\n",
    "modelES.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8176e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, the model can be fitted using the `fit` method. Notice that the following additional arguments are passed:\n",
    "\n",
    "+ The optional argument `validation_data` is used to provide the validation data and labels.\n",
    "+ The `callbacks` argument is used to pass the imported `PlotLossesKeras` object and the previously defined `callback` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee74a61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelES.fit(xTrain, \n",
    "            labelsTrain,\n",
    "            validation_data=(xVal, labelsVal),\n",
    "            callbacks=[PlotLossesKeras(), callback], \n",
    "            epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7f664",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `PlotLossesKeras` callback allows to monitor the training and validation errors in real time, while, through the use of early stopping, the difference between validation and training error can be decreased. Notice that this is achieved despite the fact that the capacity of the model used is probably to high for the current problem.\n",
    "\n",
    "The error and accuracy for the test set can be evaluated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1902b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lossTest, accTest = modelES.evaluate(xTest,  labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1414b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The decision boundary for the new model can be visualised as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfe3cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plotDecisionBoundary(modelES)\n",
    "\n",
    "ax.plot(xOF[class0OF,0],xOF[class0OF,1],'.b')\n",
    "ax.plot(xOF[class1OF,0],xOF[class1OF,1],'.r')\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9cdab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again a smoother decision boundary can be obtained despite the presence of noise. Again notice that this is possible despite the fact that the capacity of the model used is probably too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80f3a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### Key points\n",
    "\n",
    "In this lecture:\n",
    "\n",
    "+ We presented in detail the structure and training process for MLPs.\n",
    "+ We discussed about training MLPs and potential problems such as overfitting.\n",
    "+ We demonstrated some strategies for training MLPs and avoiding overfitting\n",
    "\n",
    "The above should allow us to solve today's problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98893132",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resources/Further reading\n",
    "\n",
    "+ [`keras` website](https://keras.io/)\n",
    "+ I. Goodfellow, Y. Bengio and A. Courville. *Deep learning*. MIT press, 2016 (library 006.31 GOO)\n",
    "+ D.W. Patterson. *Artificial Neural Networks: Theory and Applications*. Prentice Hall, 1996 (library 006.3 PAT)\n",
    "+ K. Mehrotra, C.K. Mohan, S. Ranka. *Elements of Artificial neural networks*, MIT Press, 1997 (library 001.535 MEH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c8865",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Solving today's problem\n",
    "\n",
    "Today's problem is a classification problem with 27 inputs and 5 classes (grade 0-4). As a first step towards it, the data should be converted to `numpy` arrays and normalised as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecaf24",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#convert data and labels (metal grades) to numpy arrays\n",
    "dataMetal = metalDf.iloc[:,:-1].to_numpy()\n",
    "labelsMetal = metalDf.iloc[:,-1].to_numpy()\n",
    "\n",
    "#normalise data using the maximum value of each column\n",
    "dataMetal = dataMetal/dataMetal.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503e5ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next we can split the data into training and testing sets using a 80%-20% split and, subsequently, further split the training data into training and validation using a 80%-20% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca00349",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677916f8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A MLP can be created to classify the data and early stopping can be employed to avoid overfitting. The number of inputs and outputs for the network should be 27 and 5 respectively, while a single hidden layer can be used. Different numbers of hidden units, for instance 50, 100, 200 can be tested manually or automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd790cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fa603d1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The model error and accuracy should be evaluated for the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce39758",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd2811ec",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, the labels of the test set can be predicted and visualised using a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d40f26",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc503a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Additional practice\n",
    "\n",
    "For some additional practice, in the cells below, the full iris data set is loaded and normalised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da87982",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd #import pandas module\n",
    "\n",
    "#pd.set_option('display.max_rows', None) #uncomment this line to view full dataset\n",
    "\n",
    "irisDf = pd.read_csv(\"iris_full.csv\") #read the file containing the data\n",
    "\n",
    "irisDf #visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0dbdc6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "irisData=irisDf[['sepal length','sepal width','petal length','petal width']].to_numpy()\n",
    "irisData = irisData/irisData.max(axis=0)\n",
    "\n",
    "irisLabels = np.zeros(150)\n",
    "irisLabels[50:100]=1\n",
    "irisLabels[100:]=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3945a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to what was shown above, the dataset can be split into training, validation and testing parts and a MLP can be trained to classify the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcafe21",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
